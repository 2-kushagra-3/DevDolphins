{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87047ae8-2c94-4ed7-987f-292069f2ba48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c78e41-5f3d-40a8-91ad-b30aa99814a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import io\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c88452-80b6-4502-8e57-9cf04065901f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_file_id(service, folder_id, filename):\n",
    "    query = f\"'{folder_id}' in parents and name='{filename}' and trashed=false\"\n",
    "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    files = results.get('files', [])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"{filename} not found in folder {folder_id}\")\n",
    "    return files[0]['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d413b7-a9d0-4936-98a2-30548e50cd67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_account_file = \"/Workspace/Users/kushagraverma@live.in/Mechanism X/rapid-compound-463822-c8-74c43978101f.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b695619e-3b1f-492a-9297-00f888600e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Setup GDrive connection (already shared)\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "        service_account_file,\n",
    "        scopes=[\"https://www.googleapis.com/auth/drive\"]\n",
    "    )\n",
    "service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Step 2: Define your folder and filenames\n",
    "folder_id = \"1qryhdlgNsmecWRy2haI8S3uC63wKk5X-\"\n",
    "transactions_file = \"transactions.csv\"\n",
    "customer_file = \"CustomerImportance.csv\"\n",
    "\n",
    "# Step 3: Download both files\n",
    "def read_csv_from_gdrive(service, file_id):\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    file_buffer = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_buffer, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        _, done = downloader.next_chunk()\n",
    "    file_buffer.seek(0)\n",
    "    pandas_df = pd.read_csv(file_buffer)\n",
    "    return spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Get file IDs\n",
    "transactions_id = get_file_id(service, folder_id, transactions_file)\n",
    "customer_id = get_file_id(service, folder_id, customer_file)\n",
    "\n",
    "# Read as PySpark DataFrames\n",
    "transactions_df = read_csv_from_gdrive(service, transactions_id)\n",
    "customer_df = read_csv_from_gdrive(service, customer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cfc951-468f-4287-961e-13d525d65840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# === Config ===\n",
    "base_path = \"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net\"\n",
    "transactions_path = f\"{base_path}/transactions\"\n",
    "reference_path = f\"{base_path}/reference_data/customer_importance\"\n",
    "\n",
    "# === Function to rename part file directly in transactions folder ===\n",
    "def rename_and_flatten_chunk(temp_dir: str, final_path: str):\n",
    "    files = dbutils.fs.ls(temp_dir)\n",
    "    for file in files:\n",
    "        if file.name.startswith(\"part-\") and file.name.endswith(\".csv\"):\n",
    "            dbutils.fs.mv(file.path, final_path)\n",
    "        else:\n",
    "            dbutils.fs.rm(file.path)\n",
    "    dbutils.fs.rm(temp_dir, recurse=True)\n",
    "\n",
    "# === Step 1: Write Reference Data ===\n",
    "customer_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(reference_path)\n",
    "print(f\"Uploaded customer importance data to: {reference_path}\")\n",
    "\n",
    "# === Step 2: Write transactions as flat CSVs directly in /transactions ===\n",
    "chunk_size = 10000\n",
    "total_rows = transactions_df.count()\n",
    "num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Add row number\n",
    "window_spec = Window.orderBy(monotonically_increasing_id())\n",
    "transactions_df = transactions_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_time = time.time()\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size\n",
    "    \n",
    "    chunk_df = transactions_df.filter((transactions_df.row_num > start) & (transactions_df.row_num <= end)).drop(\"row_num\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    temp_chunk_dir = f\"{transactions_path}/tmp_chunk_{i+1}_{timestamp}\"\n",
    "    final_file_path = f\"{transactions_path}/chunk_{i+1}_{timestamp}.csv\"\n",
    "    \n",
    "    # Write chunk to temp location\n",
    "    chunk_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_chunk_dir)\n",
    "    \n",
    "    # Move part file to final flat location\n",
    "    rename_and_flatten_chunk(temp_chunk_dir, final_file_path)\n",
    "    \n",
    "    print(f\"Uploaded chunk {i+1}/{num_chunks} as {final_file_path}\")\n",
    "    \n",
    "    # Wait 1 second\n",
    "    elapsed = time.time() - start_time\n",
    "    sleep_time = max(0, 1.0 - elapsed)\n",
    "    time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1211ea93-19be-48ba-b8c8-e30e68a001fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mech X",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
