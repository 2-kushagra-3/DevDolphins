{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4425b4e3-6c5b-4d62-9bf2-0de565d429cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pg_user = dbutils.secrets.get(scope=\"devDolphin\", key=\"kushagra\")\n",
    "pg_pass = dbutils.secrets.get(scope=\"devDolphin\", key=\"pg-password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a40c7c8-4556-4c4d-a7e5-4eb0d5d1a0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pg_user = dbutils.secrets.get(scope=\"devDolphin\", key=\"kushagra\")\n",
    "pg_pass = dbutils.secrets.get(scope=\"devDolphin\", key=\"pg-password\")\n",
    "\n",
    "hostname = \"devdolphinpostgresdb.postgres.database.azure.com\"\n",
    "database = \"postgres\"  # or your actual DB name\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{hostname}:5432/{database}?sslmode=require\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_pass,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# ✅ Read test table\n",
    "df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"information_schema.tables\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26509f46-c350-4727-b764-622526b8f80a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# 1️⃣ Create a tiny test DataFrame\n",
    "test_data = [\n",
    "    Row(id=1, name=\"Alice\"),\n",
    "    Row(id=2, name=\"Bob\")\n",
    "]\n",
    "\n",
    "test_df = spark.createDataFrame(test_data)\n",
    "\n",
    "# 2️⃣ Write to PostgreSQL\n",
    "test_table = \"test_write_permissions\"\n",
    "\n",
    "test_df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=test_table,\n",
    "    mode=\"overwrite\",  # overwrite if exists\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(f\"✅ Successfully wrote to table '{test_table}'\")\n",
    "\n",
    "# 3️⃣ Read it back to verify\n",
    "verify_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=test_table,\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "verify_df.show()\n",
    "\n",
    "# 4️⃣ Optional: Drop the table when done\n",
    "# You can drop via SQL if you want:\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {test_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda8ad73-6bb0-4a43-9e27-aab8106aaebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1️⃣ Create table schema in Spark\n",
    "empty_df = spark.createDataFrame([], \"merchant STRING, total_txn LONG\")\n",
    "\n",
    "# 2️⃣ Write it to Postgres once\n",
    "empty_df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", jdbc_url) \\\n",
    "  .option(\"dbtable\", \"merchant_txn_state\") \\\n",
    "  .option(\"user\", pg_user) \\\n",
    "  .option(\"password\", pg_pass) \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .save()\n",
    "\n",
    "print(\"✅ Table created in Postgres!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1a64d0-3d1a-4c2b-bb10-1f40915aee47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Write a mini DataFrame for testing\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Example rows: (merchant, total_txn)\n",
    "data = [Row(merchant='M001', total_txn=100), Row(merchant='M002', total_txn=250)]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", jdbc_url) \\\n",
    "  .option(\"dbtable\", \"temp_merchant_txn\") \\\n",
    "  .option(\"user\", pg_user) \\\n",
    "  .option(\"password\", pg_pass) \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .save()\n",
    "\n",
    "print(\"✅ Temp table written to Postgres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e323879-397b-4cc3-a7f4-14634aedd798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd8a3c7-8a39-46aa-92d9-2723e887a9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=hostname,\n",
    "    database=database,\n",
    "    user=pg_user,\n",
    "    password=pg_pass,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Drop if exists and recreate with PRIMARY KEY\n",
    "cur.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS merchant_txn_state;\n",
    "    CREATE TABLE merchant_txn_state (\n",
    "        merchant TEXT PRIMARY KEY,\n",
    "        total_txn BIGINT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"✅ merchant_txn_state created with PRIMARY KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2414d7-98cb-4044-b417-7d8b4a0728da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "merge_sql = \"\"\"\n",
    "INSERT INTO merchant_txn_state (merchant, total_txn)\n",
    "SELECT merchant, total_txn FROM temp_merchant_txn\n",
    "ON CONFLICT (merchant)\n",
    "DO UPDATE SET total_txn = EXCLUDED.total_txn;\n",
    "\"\"\"\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=hostname,\n",
    "    database=database,\n",
    "    user=pg_user,\n",
    "    password=pg_pass,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(merge_sql)\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"✅ Upsert done via psycopg2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a35c51-a4b4-4ca8-b742-40c9e5af6046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the final upserted table\n",
    "df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"merchant_txn_state\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3b77f7-b26b-47fb-bf10-326624aa1c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "query_sql = \"SELECT * FROM merchant_txn_state LIMIT 20;\"\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=hostname,\n",
    "    database=database,\n",
    "    user=pg_user,\n",
    "    password=pg_pass,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(query_sql)\n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a793d11-3673-4a37-ad95-c00f9127be07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pg_user = dbutils.secrets.get(scope=\"devDolphin\", key=\"kushagra\")\n",
    "pg_pass = dbutils.secrets.get(scope=\"devDolphin\", key=\"pg-password\")\n",
    "\n",
    "hostname = \"devdolphinpostgresdb.postgres.database.azure.com\"\n",
    "database = \"postgres\"  # or your actual DB name\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{hostname}:5432/{database}?sslmode=require\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_pass,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# === 1) Create empty DataFrames with required schema ===\n",
    "\n",
    "gender_state_schema = \"merchant STRING, customer STRING, gender STRING\"\n",
    "pattern3_schema = \"\"\"\n",
    "  YStartTime TIMESTAMP,\n",
    "  detectionTime TIMESTAMP,\n",
    "  patternId STRING,\n",
    "  ActionType STRING,\n",
    "  customerName STRING,\n",
    "  MerchantId STRING\n",
    "\"\"\"\n",
    "\n",
    "empty_gender_state = spark.createDataFrame([], schema=gender_state_schema)\n",
    "empty_pattern3 = spark.createDataFrame([], schema=pattern3_schema)\n",
    "\n",
    "# === 2) Write them once with mode \"overwrite\" to create tables if not exist ===\n",
    "\n",
    "empty_gender_state.write.mode(\"overwrite\").jdbc(\n",
    "    jdbc_url,\n",
    "    \"gender_summary_state\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "empty_pattern3.write.mode(\"overwrite\").jdbc(\n",
    "    jdbc_url,\n",
    "    \"pattern3_detections\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "print(\"✅ Both Postgres tables created or overwritten.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a7e48c-ac61-431f-8dd7-e4c17e3488c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "query_sql = \"SELECT * FROM gender_summary_state ;\"\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=hostname,\n",
    "    database=database,\n",
    "    user=pg_user,\n",
    "    password=pg_pass,\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(query_sql)\n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a29c911c-0047-41fb-8622-154a0c6c56be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "postgres set up",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
