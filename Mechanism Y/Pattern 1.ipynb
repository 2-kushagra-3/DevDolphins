{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc0aaa7c-defe-48ad-9fbb-cc9a6ce26ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, when, count, mean, expr\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "customer_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .csv(\"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/reference_data/customer data.csv\")\n",
    ")\n",
    "\n",
    "merchant_transaction_count_df = None\n",
    "customer_merchant_stats_df = None\n",
    "already_detected_pat1 = set()\n",
    "\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/schema/ChunksSchema/\")\n",
    "    .load(\"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/transactions/\")\n",
    ")\n",
    "\n",
    "def foreach_batch_function(batch_df, batch_id):\n",
    "    global merchant_transaction_count_df, customer_merchant_stats_df, already_detected_pat1\n",
    "\n",
    "    print(f\"\\n Processing batch {batch_id} rows: {batch_df.count()}\")\n",
    "\n",
    "    # Join with reference customer_df\n",
    "    merged_df = batch_df.join(\n",
    "        customer_df,\n",
    "        (batch_df[\"customer\"] == customer_df[\"Source\"]) &\n",
    "        (batch_df[\"merchant\"] == customer_df[\"Target\"]) &\n",
    "        (batch_df[\"category\"] == customer_df[\"typeTrans\"]) &\n",
    "        (batch_df[\"amount\"] == customer_df[\"Weight\"]),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    print(f\" Merged rows: {merged_df.count()}\")\n",
    "\n",
    "  \n",
    "    merchant_txn = (\n",
    "        merged_df.groupBy(\"merchant\")\n",
    "        .agg(count(\"*\").alias(\"new_txn_count\"))\n",
    "    )\n",
    "\n",
    "    if merchant_transaction_count_df is None:\n",
    "        merchant_transaction_count_df = merchant_txn.withColumnRenamed(\"new_txn_count\", \"total_txn\")\n",
    "    else:\n",
    "        left = merchant_transaction_count_df\n",
    "        right = merchant_txn\n",
    "\n",
    "        merchant_transaction_count_df = (\n",
    "            left.join(right, on=\"merchant\", how=\"outer\")\n",
    "            .na.fill(0)\n",
    "            .withColumn(\"total_txn\", col(\"total_txn\") + col(\"new_txn_count\"))\n",
    "            .select(\"merchant\", \"total_txn\")\n",
    "        )\n",
    "\n",
    "    print(f\" Updated merchant_transaction_count_df count: {merchant_transaction_count_df.count()}\")\n",
    "\n",
    "\n",
    "    cust_merchant_stats = (\n",
    "        merged_df.groupBy(\"customer\", \"merchant\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"new_txn_count\"),\n",
    "            mean(\"Weight\").alias(\"new_avg_weight\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if customer_merchant_stats_df is None:\n",
    "        customer_merchant_stats_df = cust_merchant_stats.withColumnRenamed(\"new_txn_count\", \"txn_count\") \\\n",
    "                                                         .withColumnRenamed(\"new_avg_weight\", \"avg_weight\")\n",
    "    else:\n",
    "        left = customer_merchant_stats_df\n",
    "        right = cust_merchant_stats\n",
    "\n",
    "        combined = (\n",
    "            left.join(right, on=[\"customer\", \"merchant\"], how=\"outer\")\n",
    "            .na.fill(0)\n",
    "            .withColumn(\"txn_count\", col(\"txn_count\") + col(\"new_txn_count\"))\n",
    "            .withColumn(\n",
    "                \"avg_weight\",\n",
    "                when(col(\"txn_count\") == 0, 0).otherwise(\n",
    "                    (col(\"avg_weight\") * col(\"txn_count\") + col(\"new_avg_weight\") * col(\"new_txn_count\")) /\n",
    "                    (col(\"txn_count\") + col(\"new_txn_count\"))\n",
    "                )\n",
    "            )\n",
    "            .select(\"customer\", \"merchant\", \"txn_count\", \"avg_weight\")\n",
    "        )\n",
    "\n",
    "        customer_merchant_stats_df = combined\n",
    "\n",
    "    print(f\" Updated customer_merchant_stats_df count: {customer_merchant_stats_df.count()}\")\n",
    "\n",
    "\n",
    "    eligible_merchants = merchant_transaction_count_df.filter(col(\"total_txn\") > 50000).select(\"merchant\").distinct()\n",
    "    eligible_merchants_list = [row[\"merchant\"] for row in eligible_merchants.collect()]\n",
    "\n",
    "    detections = []\n",
    "    for merchant in eligible_merchants_list:\n",
    "        cust_stats = customer_merchant_stats_df.filter(col(\"merchant\") == merchant)\n",
    "\n",
    "        if cust_stats.count() == 0:\n",
    "            continue\n",
    "\n",
    "        txn_thresh = cust_stats.approxQuantile(\"txn_count\", [0.9], 0.0)[0]\n",
    "        weight_thresh = cust_stats.approxQuantile(\"avg_weight\", [0.1], 0.0)[0]\n",
    "\n",
    "        detected = cust_stats.filter(\n",
    "            (col(\"txn_count\") >= txn_thresh) & (col(\"avg_weight\") <= weight_thresh)\n",
    "        ).select(\"customer\", \"merchant\")\n",
    "\n",
    "        for row in detected.collect():\n",
    "            key = (row[\"customer\"], row[\"merchant\"])\n",
    "            if key in already_detected_pat1:\n",
    "                continue\n",
    "\n",
    "            detections.append({\n",
    "                \"YStartTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"detectionTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"patternId\": \"PatId1\",\n",
    "                \"ActionType\": \"UPGRADE\",\n",
    "                \"customerName\": row[\"customer\"],\n",
    "                \"MerchantId\": row[\"merchant\"]\n",
    "            })\n",
    "            already_detected_pat1.add(key)\n",
    "\n",
    "    if detections:\n",
    "        detections_df = spark.createDataFrame(detections)\n",
    "        detections_df.show(truncate=False)\n",
    "\n",
    "        detections_df.write.jdbc(jdbc_url, \"already_detected_pat1\", mode=\"append\", properties=jdbc_props)\n",
    "\n",
    "        print(f\" Wrote {len(detections)} Pattern 1 detections to Postgres.\")\n",
    "    else:\n",
    "        print(\" No new detections for Pattern 1.\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# âœ… Start Stream\n",
    "# ----------------------------------------\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .foreachBatch(foreach_batch_function)\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/checkpoints/pat1/\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pattern 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
