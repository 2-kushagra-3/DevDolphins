{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7707313c-2773-4819-96ce-19b8305f0d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pg_user = dbutils.secrets.get(scope=\"devDolphin\", key=\"kushagra\")\n",
    "pg_pass = dbutils.secrets.get(scope=\"devDolphin\", key=\"pg-password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981bc260-a392-43b8-94d7-6923aa470482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, input_file_name\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "\n",
    "hostname = \"devdolphinpostgresdb.postgres.database.azure.com\"\n",
    "database = \"postgres\"\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{hostname}:5432/{database}?sslmode=require\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": pg_user,\n",
    "    \"password\": pg_pass,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# === ✅ Utility ===\n",
    "def create_empty_tables():\n",
    "    # Empty schema DFs\n",
    "    gender_state_schema = \"merchant STRING, customer STRING, gender STRING\"\n",
    "    pattern3_schema = \"\"\"\n",
    "      YStartTime TIMESTAMP,\n",
    "      detectionTime TIMESTAMP,\n",
    "      patternId STRING,\n",
    "      ActionType STRING,\n",
    "      customerName STRING,\n",
    "      MerchantId STRING\n",
    "    \"\"\"\n",
    "\n",
    "    spark.createDataFrame([], gender_state_schema) \\\n",
    "        .write.mode(\"overwrite\").jdbc(jdbc_url, \"gender_summary_state\", properties=connection_properties)\n",
    "\n",
    "    spark.createDataFrame([], pattern3_schema) \\\n",
    "        .write.mode(\"overwrite\").jdbc(jdbc_url, \"pattern3_detections\", properties=connection_properties)\n",
    "\n",
    "    print(\"✅ Tables created in Postgres.\")\n",
    "\n",
    "# Call once if needed:\n",
    "# create_empty_tables()\n",
    "\n",
    "# === ✅ Pattern 3 logic ===\n",
    "def pattern3_detect(batch_df, batch_id):\n",
    "    print(f\"\\n🔍 New batch {batch_id}\")\n",
    "    print(f\"Rows: {batch_df.count()}\")\n",
    "\n",
    "    # Clean gender\n",
    "    batch_df = batch_df.withColumn(\"gender\", F.upper(F.trim(F.col(\"gender\"))))\n",
    "\n",
    "    # Keep required cols\n",
    "    chunk_df = batch_df.select(\"merchant\", \"customer\", \"gender\").dropna()\n",
    "\n",
    "    # --- Load current state ---\n",
    "    gender_state_df = spark.read.jdbc(jdbc_url, \"gender_summary_state\", properties=connection_properties)\n",
    "\n",
    "    # Union new data & dedupe\n",
    "    updated_state_df = (\n",
    "        gender_state_df.unionByName(chunk_df)\n",
    "        .dropDuplicates([\"merchant\", \"customer\", \"gender\"])\n",
    "    )\n",
    "\n",
    "    # === Gender conflict removal ===\n",
    "    gender_counts = (\n",
    "        updated_state_df.groupBy(\"merchant\", \"customer\")\n",
    "        .agg(F.countDistinct(\"gender\").alias(\"gender_count\"))\n",
    "    )\n",
    "\n",
    "    conflict_keys = (\n",
    "        gender_counts.filter(col(\"gender_count\") > 1)\n",
    "        .select(\"merchant\", \"customer\")\n",
    "    )\n",
    "\n",
    "    # Remove conflicts\n",
    "    conflict_keys = conflict_keys.withColumn(\"join_key\", F.concat_ws(\"_\", \"merchant\", \"customer\"))\n",
    "    updated_state_df = updated_state_df.withColumn(\"join_key\", F.concat_ws(\"_\", \"merchant\", \"customer\"))\n",
    "\n",
    "    clean_state_df = updated_state_df.join(conflict_keys, on=\"join_key\", how=\"left_anti\").drop(\"join_key\")\n",
    "\n",
    "    # === Gender summary pivot ===\n",
    "    gender_group = (\n",
    "        clean_state_df.groupBy(\"merchant\", \"gender\")\n",
    "        .agg(F.countDistinct(\"customer\").alias(\"customer_count\"))\n",
    "    )\n",
    "\n",
    "    pivot_df = gender_group.groupBy(\"merchant\") \\\n",
    "        .pivot(\"gender\", [\"F\", \"M\"]) \\\n",
    "        .sum(\"customer_count\") \\\n",
    "        .fillna(0)\n",
    "\n",
    "    # Eligible pattern 3\n",
    "    eligible_df = pivot_df.filter((col(\"F\") > 100) & (col(\"F\") < col(\"M\")))\n",
    "\n",
    "    if eligible_df.count() == 0:\n",
    "        print(\"✅ No new Pattern 3 merchants.\")\n",
    "    else:\n",
    "        print(\"🌟 Eligible DEI-NEEDED merchants:\")\n",
    "        eligible_df.show()\n",
    "\n",
    "        now = datetime.now()\n",
    "\n",
    "        detections_df = eligible_df.withColumn(\"YStartTime\", F.lit(now)) \\\n",
    "            .withColumn(\"detectionTime\", F.lit(now)) \\\n",
    "            .withColumn(\"patternId\", F.lit(\"PatId3\")) \\\n",
    "            .withColumn(\"ActionType\", F.lit(\"DEI-NEEDED\")) \\\n",
    "            .withColumn(\"customerName\", F.lit(\"\")) \\\n",
    "            .withColumnRenamed(\"merchant\", \"MerchantId\") \\\n",
    "            .select(\n",
    "                \"YStartTime\", \"detectionTime\", \"patternId\",\n",
    "                \"ActionType\", \"customerName\", \"MerchantId\"\n",
    "            )\n",
    "\n",
    "        detections_df.write.mode(\"append\").jdbc(jdbc_url, \"pattern3_detections\", properties=connection_properties)\n",
    "        print(f\"✅ Inserted {detections_df.count()} new detections.\")\n",
    "\n",
    "    # Write back cleaned state\n",
    "    clean_state_df.write.mode(\"overwrite\").jdbc(jdbc_url, \"gender_summary_state\", properties=connection_properties)\n",
    "    print(f\"✅ State table updated. Rows: {clean_state_df.count()}\")\n",
    "\n",
    "\n",
    "# === ✅ Start AutoLoader Stream ===\n",
    "source_path = \"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/transactions\"\n",
    "schema_location = \"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/_schema/transactions\"\n",
    "\n",
    "stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .load(source_path)\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# === ✅ Hook up foreachBatch ===\n",
    "query = (\n",
    "    stream_df.writeStream\n",
    "    .foreachBatch(pattern3_detect)\n",
    "    .option(\"checkpointLocation\", \"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/_checkpoint/pattern3\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f97aae59-22e2-441f-975a-6d59cb52ef50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "#  Start Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#  Load CSV (adjust your path)\n",
    "df = spark.read.csv(\"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/transactions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(f\" Total rows loaded: {df.count()}\")\n",
    "\n",
    "#  Step 0: Clean gender (remove quotes, trim, upper)\n",
    "df_clean = (\n",
    "    df.withColumn(\n",
    "        \"gender\",\n",
    "        F.upper(F.trim(F.regexp_replace(\"gender\", \"'\", \"\")))\n",
    "    )\n",
    ")\n",
    "\n",
    "#  Step 1: Deduplicate (merchant, customer, gender)\n",
    "deduped = (\n",
    "    df_clean\n",
    "    .select(\"merchant\", \"customer\", \"gender\")\n",
    "    .dropna()\n",
    "    .dropDuplicates([\"merchant\", \"customer\", \"gender\"])\n",
    ")\n",
    "\n",
    "print(f\"Deduped unique rows: {deduped.count()}\")\n",
    "\n",
    "# Step 2: Group by merchant + gender, count unique customers\n",
    "gender_counts = (\n",
    "    deduped\n",
    "    .groupBy(\"merchant\", \"gender\")\n",
    "    .agg(F.countDistinct(\"customer\").alias(\"customer_count\"))\n",
    ")\n",
    "\n",
    "# Step 3: Pivot gender counts\n",
    "pivot = (\n",
    "    gender_counts\n",
    "    .groupBy(\"merchant\")\n",
    "    .pivot(\"gender\", [\"F\", \"M\"])\n",
    "    .sum(\"customer_count\")\n",
    "    .fillna(0)\n",
    "    .withColumnRenamed(\"F\", \"female_count\")\n",
    "    .withColumnRenamed(\"M\", \"male_count\")\n",
    ")\n",
    "\n",
    "print(\"📊 Pivot sample:\")\n",
    "pivot.show(10, truncate=False)\n",
    "\n",
    "# ✅ Step 4: Filter for pattern 3\n",
    "filtered = pivot.filter(\n",
    "    (F.col(\"female_count\") > 100) & (F.col(\"female_count\") < F.col(\"male_count\"))\n",
    ")\n",
    "\n",
    "print(\"Eligible DEI-NEEDED merchants:\")\n",
    "filtered.show(truncate=False)\n",
    "\n",
    "# Step 5: Construct detections DataFrame\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "detections = (\n",
    "    filtered\n",
    "    .withColumn(\"YStartTime\", F.lit(now))\n",
    "    .withColumn(\"detectionTime\", F.lit(now))\n",
    "    .withColumn(\"patternId\", F.lit(\"PatId3\"))\n",
    "    .withColumn(\"ActionType\", F.lit(\"DEI-NEEDED\"))\n",
    "    .withColumn(\"customerName\", F.lit(\"\"))\n",
    "    .select(\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customerName\", \"merchant\")\n",
    "    .withColumnRenamed(\"merchant\", \"MerchantId\")\n",
    ")\n",
    "\n",
    "print(f\"Total detections: {detections.count()}\")\n",
    "detections.show(truncate=False)\n",
    "\n",
    "# Save or inspect as needed\n",
    "detections.write.csv(\"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net/p3/\", header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"🎉 Finished! \")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pattern 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
