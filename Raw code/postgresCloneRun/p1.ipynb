{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbafb730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Processing: chunk_10_20250624_202551.csv\n",
      "\n",
      "📦 Processing: chunk_11_20250624_202553.csv\n",
      "\n",
      "📦 Processing: chunk_12_20250624_202555.csv\n",
      "\n",
      "📦 Processing: chunk_13_20250624_202557.csv\n",
      "\n",
      "📦 Processing: chunk_14_20250624_202559.csv\n",
      "\n",
      "📦 Processing: chunk_15_20250624_202601.csv\n",
      "\n",
      "📦 Processing: chunk_16_20250624_202602.csv\n",
      "\n",
      "📦 Processing: chunk_17_20250624_202608.csv\n",
      "\n",
      "📦 Processing: chunk_18_20250624_202610.csv\n",
      "\n",
      "📦 Processing: chunk_19_20250624_202612.csv\n",
      "\n",
      "📦 Processing: chunk_1_20250624_202535.csv\n",
      "✅ Wrote 4 detections to outputs//detections_pat1_20250627_043847_634903.csv\n",
      "\n",
      "📦 Processing: chunk_20_20250624_202614.csv\n",
      "✅ Wrote 3 detections to outputs//detections_pat1_20250627_043847_833818.csv\n",
      "\n",
      "📦 Processing: chunk_21_20250624_202616.csv\n",
      "✅ Wrote 16 detections to outputs//detections_pat1_20250627_043848_044608.csv\n",
      "\n",
      "📦 Processing: chunk_22_20250624_202618.csv\n",
      "✅ Wrote 13 detections to outputs//detections_pat1_20250627_043848_268870.csv\n",
      "\n",
      "📦 Processing: chunk_23_20250624_202619.csv\n",
      "✅ Wrote 16 detections to outputs//detections_pat1_20250627_043848_462750.csv\n",
      "\n",
      "📦 Processing: chunk_24_20250624_202621.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043848_670507.csv\n",
      "\n",
      "📦 Processing: chunk_25_20250624_202623.csv\n",
      "✅ Wrote 15 detections to outputs//detections_pat1_20250627_043848_860474.csv\n",
      "\n",
      "📦 Processing: chunk_26_20250624_202625.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043849_097888.csv\n",
      "\n",
      "📦 Processing: chunk_27_20250624_202627.csv\n",
      "✅ Wrote 10 detections to outputs//detections_pat1_20250627_043849_296850.csv\n",
      "\n",
      "📦 Processing: chunk_28_20250624_202629.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043849_527580.csv\n",
      "\n",
      "📦 Processing: chunk_29_20250624_202631.csv\n",
      "✅ Wrote 9 detections to outputs//detections_pat1_20250627_043849_748745.csv\n",
      "\n",
      "📦 Processing: chunk_2_20250624_202537.csv\n",
      "✅ Wrote 14 detections to outputs//detections_pat1_20250627_043849_963536.csv\n",
      "\n",
      "📦 Processing: chunk_30_20250624_202633.csv\n",
      "✅ Wrote 11 detections to outputs//detections_pat1_20250627_043850_178926.csv\n",
      "\n",
      "📦 Processing: chunk_31_20250624_202635.csv\n",
      "✅ Wrote 11 detections to outputs//detections_pat1_20250627_043850_399258.csv\n",
      "\n",
      "📦 Processing: chunk_32_20250624_202637.csv\n",
      "✅ Wrote 11 detections to outputs//detections_pat1_20250627_043850_611037.csv\n",
      "\n",
      "📦 Processing: chunk_33_20250624_202639.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043850_850304.csv\n",
      "\n",
      "📦 Processing: chunk_34_20250624_202640.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043851_066677.csv\n",
      "\n",
      "📦 Processing: chunk_35_20250624_202642.csv\n",
      "✅ Wrote 13 detections to outputs//detections_pat1_20250627_043851_284854.csv\n",
      "✅ Wrote 1 detections to outputs//detections_pat2_20250627_043851_284854.csv\n",
      "\n",
      "📦 Processing: chunk_36_20250624_202644.csv\n",
      "✅ Wrote 14 detections to outputs//detections_pat1_20250627_043851_514782.csv\n",
      "\n",
      "📦 Processing: chunk_37_20250624_202646.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043851_719603.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043851_728365.csv\n",
      "\n",
      "📦 Processing: chunk_38_20250624_202648.csv\n",
      "✅ Wrote 13 detections to outputs//detections_pat1_20250627_043851_944020.csv\n",
      "✅ Wrote 3 detections to outputs//detections_pat2_20250627_043851_945136.csv\n",
      "\n",
      "📦 Processing: chunk_39_20250624_202650.csv\n",
      "✅ Wrote 14 detections to outputs//detections_pat1_20250627_043852_166166.csv\n",
      "✅ Wrote 3 detections to outputs//detections_pat2_20250627_043852_166166.csv\n",
      "\n",
      "📦 Processing: chunk_3_20250624_202539.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043852_386091.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043852_387098.csv\n",
      "\n",
      "📦 Processing: chunk_40_20250624_202652.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043852_600166.csv\n",
      "✅ Wrote 7 detections to outputs//detections_pat2_20250627_043852_600166.csv\n",
      "\n",
      "📦 Processing: chunk_41_20250624_202653.csv\n",
      "✅ Wrote 11 detections to outputs//detections_pat1_20250627_043852_852323.csv\n",
      "✅ Wrote 4 detections to outputs//detections_pat2_20250627_043852_853321.csv\n",
      "\n",
      "📦 Processing: chunk_42_20250624_202655.csv\n",
      "✅ Wrote 13 detections to outputs//detections_pat1_20250627_043853_107438.csv\n",
      "✅ Wrote 5 detections to outputs//detections_pat2_20250627_043853_109526.csv\n",
      "\n",
      "📦 Processing: chunk_43_20250624_202657.csv\n",
      "✅ Wrote 9 detections to outputs//detections_pat1_20250627_043853_338559.csv\n",
      "✅ Wrote 7 detections to outputs//detections_pat2_20250627_043853_338559.csv\n",
      "\n",
      "📦 Processing: chunk_44_20250624_202659.csv\n",
      "✅ Wrote 10 detections to outputs//detections_pat1_20250627_043853_581777.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043853_583775.csv\n",
      "\n",
      "📦 Processing: chunk_45_20250624_202701.csv\n",
      "✅ Wrote 13 detections to outputs//detections_pat1_20250627_043853_803906.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043853_809495.csv\n",
      "\n",
      "📦 Processing: chunk_46_20250624_202703.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043854_030801.csv\n",
      "✅ Wrote 5 detections to outputs//detections_pat2_20250627_043854_030801.csv\n",
      "\n",
      "📦 Processing: chunk_47_20250624_202705.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043854_268129.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043854_268129.csv\n",
      "\n",
      "📦 Processing: chunk_48_20250624_202707.csv\n",
      "✅ Wrote 10 detections to outputs//detections_pat1_20250627_043854_501280.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043854_503285.csv\n",
      "\n",
      "📦 Processing: chunk_49_20250624_202708.csv\n",
      "✅ Wrote 12 detections to outputs//detections_pat1_20250627_043854_731397.csv\n",
      "✅ Wrote 5 detections to outputs//detections_pat2_20250627_043854_731397.csv\n",
      "\n",
      "📦 Processing: chunk_4_20250624_202540.csv\n",
      "✅ Wrote 13 detections to outputs//detections_pat1_20250627_043854_959482.csv\n",
      "✅ Wrote 1 detections to outputs//detections_pat2_20250627_043854_961022.csv\n",
      "\n",
      "📦 Processing: chunk_50_20250624_202710.csv\n",
      "✅ Wrote 14 detections to outputs//detections_pat1_20250627_043855_182905.csv\n",
      "✅ Wrote 4 detections to outputs//detections_pat2_20250627_043855_185261.csv\n",
      "\n",
      "📦 Processing: chunk_51_20250624_202712.csv\n",
      "✅ Wrote 14 detections to outputs//detections_pat1_20250627_043855_418282.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043855_418282.csv\n",
      "\n",
      "📦 Processing: chunk_52_20250624_202713.csv\n",
      "✅ Wrote 16 detections to outputs//detections_pat1_20250627_043855_651579.csv\n",
      "\n",
      "📦 Processing: chunk_53_20250624_202715.csv\n",
      "✅ Wrote 14 detections to outputs//detections_pat1_20250627_043855_890816.csv\n",
      "\n",
      "📦 Processing: chunk_54_20250624_202717.csv\n",
      "✅ Wrote 14 detections to outputs//detections_pat1_20250627_043856_128899.csv\n",
      "✅ Wrote 8 detections to outputs//detections_pat2_20250627_043856_130244.csv\n",
      "\n",
      "📦 Processing: chunk_55_20250624_202719.csv\n",
      "✅ Wrote 17 detections to outputs//detections_pat1_20250627_043856_359338.csv\n",
      "✅ Wrote 3 detections to outputs//detections_pat2_20250627_043856_363886.csv\n",
      "\n",
      "📦 Processing: chunk_56_20250624_202721.csv\n",
      "✅ Wrote 17 detections to outputs//detections_pat1_20250627_043856_627933.csv\n",
      "✅ Wrote 3 detections to outputs//detections_pat2_20250627_043856_627933.csv\n",
      "\n",
      "📦 Processing: chunk_57_20250624_202722.csv\n",
      "✅ Wrote 18 detections to outputs//detections_pat1_20250627_043856_866546.csv\n",
      "✅ Wrote 5 detections to outputs//detections_pat2_20250627_043856_870132.csv\n",
      "\n",
      "📦 Processing: chunk_58_20250624_202724.csv\n",
      "✅ Wrote 16 detections to outputs//detections_pat1_20250627_043857_099970.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043857_099970.csv\n",
      "\n",
      "📦 Processing: chunk_59_20250624_202726.csv\n",
      "✅ Wrote 13 detections to outputs//detections_pat1_20250627_043857_332618.csv\n",
      "✅ Wrote 1 detections to outputs//detections_pat2_20250627_043857_344111.csv\n",
      "\n",
      "📦 Processing: chunk_5_20250624_202542.csv\n",
      "✅ Wrote 19 detections to outputs//detections_pat1_20250627_043857_586004.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043857_588005.csv\n",
      "\n",
      "📦 Processing: chunk_60_20250624_202728.csv\n",
      "✅ Wrote 16 detections to outputs//detections_pat1_20250627_043857_816965.csv\n",
      "✅ Wrote 1 detections to outputs//detections_pat2_20250627_043857_817693.csv\n",
      "\n",
      "📦 Processing: chunk_6_20250624_202544.csv\n",
      "✅ Wrote 19 detections to outputs//detections_pat1_20250627_043858_047610.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043858_047610.csv\n",
      "\n",
      "📦 Processing: chunk_7_20250624_202546.csv\n",
      "✅ Wrote 18 detections to outputs//detections_pat1_20250627_043858_281173.csv\n",
      "✅ Wrote 3 detections to outputs//detections_pat2_20250627_043858_281173.csv\n",
      "\n",
      "📦 Processing: chunk_8_20250624_202548.csv\n",
      "✅ Wrote 23 detections to outputs//detections_pat1_20250627_043858_536721.csv\n",
      "✅ Wrote 2 detections to outputs//detections_pat2_20250627_043858_538720.csv\n",
      "\n",
      "📦 Processing: chunk_9_20250624_202550.csv\n",
      "✅ Wrote 21 detections to outputs//detections_pat1_20250627_043858_780041.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# 🧠 Simulated PostgreSQL-like Tables\n",
    "# -----------------------------\n",
    "merchant_transaction_count_df = pd.DataFrame(columns=[\"merchant\", \"total_txn\"])\n",
    "customer_merchant_stats_df = pd.DataFrame(columns=[\"customer\", \"merchant\", \"txn_count\", \"avg_weight\"])\n",
    "customer_merchant_amount_df = pd.DataFrame(columns=[\"customer\", \"merchant\", \"txn_count\", \"total_amount\"])\n",
    "\n",
    "already_detected_pat2 = set()\n",
    "already_detected_pat3 = set()\n",
    "\n",
    "# -----------------------------\n",
    "# 📥 Load Customer Importance\n",
    "# -----------------------------\n",
    "customer_df = pd.read_csv(r\"C:\\Users\\kusha\\OneDrive\\Desktop\\Projects\\DevDolphins\\Blob files\\customer data\\customer data.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 🔄 Process One Chunk (State Updates)\n",
    "# -----------------------------\n",
    "def process_chunk(chunk_df):\n",
    "    global merchant_transaction_count_df\n",
    "    global customer_merchant_stats_df\n",
    "    global customer_merchant_amount_df\n",
    "\n",
    "    # Step 1: Join with customer importance\n",
    "    merged_df = chunk_df.merge(\n",
    "        customer_df,\n",
    "        how='inner',\n",
    "        left_on=['customer', 'merchant', 'category', 'amount'],\n",
    "        right_on=['Source', 'Target', 'typeTrans', 'Weight']\n",
    "    )\n",
    "\n",
    "    # Step 2: Update merchant transaction count\n",
    "    merchant_txn = merged_df.groupby('merchant').size().reset_index(name='new_txn_count')\n",
    "\n",
    "    if merchant_transaction_count_df.empty:\n",
    "        merchant_transaction_count_df = merchant_txn.rename(columns={'new_txn_count': 'total_txn'}).copy()\n",
    "    else:\n",
    "        merchant_transaction_count_df = pd.merge(\n",
    "            merchant_transaction_count_df,\n",
    "            merchant_txn,\n",
    "            on='merchant',\n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "        merchant_transaction_count_df['total_txn'] += merchant_transaction_count_df['new_txn_count']\n",
    "        merchant_transaction_count_df.drop(columns=['new_txn_count'], inplace=True)\n",
    "\n",
    "    # Step 3: Update Pattern 1 stats\n",
    "    cust_merchant_stats = merged_df.groupby(['customer', 'merchant']).agg(\n",
    "        txn_count=('step', 'count'),\n",
    "        avg_weight=('Weight', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    if customer_merchant_stats_df.empty:\n",
    "        customer_merchant_stats_df = cust_merchant_stats.copy()\n",
    "    else:\n",
    "        customer_merchant_stats_df = pd.merge(\n",
    "            customer_merchant_stats_df,\n",
    "            cust_merchant_stats,\n",
    "            on=['customer', 'merchant'],\n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "\n",
    "        customer_merchant_stats_df['txn_count'] = (\n",
    "            customer_merchant_stats_df['txn_count_x'] + customer_merchant_stats_df['txn_count_y']\n",
    "        )\n",
    "        customer_merchant_stats_df['avg_weight'] = (\n",
    "            (customer_merchant_stats_df['avg_weight_x'] * customer_merchant_stats_df['txn_count_x']) +\n",
    "            (customer_merchant_stats_df['avg_weight_y'] * customer_merchant_stats_df['txn_count_y'])\n",
    "        ) / customer_merchant_stats_df['txn_count']\n",
    "\n",
    "        customer_merchant_stats_df = customer_merchant_stats_df[[\n",
    "            'customer', 'merchant', 'txn_count', 'avg_weight'\n",
    "        ]].copy()\n",
    "\n",
    "    # Step 4: Update Pattern 2 stats\n",
    "    cust_merchant_amount_stats = chunk_df.groupby(['customer', 'merchant']).agg(\n",
    "        txn_count=('amount', 'count'),\n",
    "        total_amount=('amount', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    if customer_merchant_amount_df.empty:\n",
    "        customer_merchant_amount_df = cust_merchant_amount_stats.copy()\n",
    "    else:\n",
    "        combined = pd.merge(\n",
    "            customer_merchant_amount_df,\n",
    "            cust_merchant_amount_stats,\n",
    "            on=['customer', 'merchant'],\n",
    "            how='outer'\n",
    "        ).fillna(0)\n",
    "\n",
    "        combined['txn_count'] = combined['txn_count_x'] + combined['txn_count_y']\n",
    "        combined['total_amount'] = combined['total_amount_x'] + combined['total_amount_y']\n",
    "\n",
    "        customer_merchant_amount_df = combined[[\n",
    "            'customer', 'merchant', 'txn_count', 'total_amount'\n",
    "        ]].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 🔍 Pattern 1\n",
    "# -----------------------------\n",
    "def detect_patid1():\n",
    "    detections = []\n",
    "    for merchant in merchant_transaction_count_df.itertuples():\n",
    "        if merchant.total_txn < 50000:\n",
    "            continue\n",
    "\n",
    "        merchant_name = merchant.merchant\n",
    "        cust_subset = customer_merchant_stats_df[\n",
    "            customer_merchant_stats_df['merchant'] == merchant_name\n",
    "        ]\n",
    "\n",
    "        if cust_subset.empty:\n",
    "            continue\n",
    "\n",
    "        txn_threshold = cust_subset['txn_count'].quantile(0.90)\n",
    "        weight_threshold = cust_subset['avg_weight'].quantile(0.10)\n",
    "\n",
    "        eligible = cust_subset[\n",
    "            (cust_subset['txn_count'] >= txn_threshold) &\n",
    "            (cust_subset['avg_weight'] <= weight_threshold)\n",
    "        ]\n",
    "\n",
    "        for row in eligible.itertuples():\n",
    "            detections.append({\n",
    "                \"YStartTime\": \"\",\n",
    "                \"detectionTime\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"patternId\": \"PatId1\",\n",
    "                \"ActionType\": \"UPGRADE\",\n",
    "                \"customerName\": row.customer,\n",
    "                \"MerchantId\": row.merchant\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(detections)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔍 Pattern 2\n",
    "# -----------------------------\n",
    "def detect_patid2():\n",
    "    detections = []\n",
    "    customer_merchant_amount_df['avg_amount'] = (\n",
    "        customer_merchant_amount_df['total_amount'] / customer_merchant_amount_df['txn_count']\n",
    "    )\n",
    "\n",
    "    filtered = customer_merchant_amount_df[\n",
    "        (customer_merchant_amount_df['txn_count'] >= 80) &\n",
    "        (customer_merchant_amount_df['avg_amount'] < 23)\n",
    "    ]\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    for _, row in filtered.iterrows():\n",
    "        key = (row['customer'], row['merchant'])\n",
    "        if key in already_detected_pat2:\n",
    "            continue\n",
    "\n",
    "        detections.append({\n",
    "            \"YStartTime\": now,\n",
    "            \"detectionTime\": now,\n",
    "            \"patternId\": \"PatId2\",\n",
    "            \"ActionType\": \"CHILD\",\n",
    "            \"customerName\": row['customer'],\n",
    "            \"MerchantId\": row['merchant'],\n",
    "            \"txn_count\": int(row['txn_count']),\n",
    "        })\n",
    "        already_detected_pat2.add(key)\n",
    "\n",
    "    return pd.DataFrame(detections)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔍 Pattern 3\n",
    "# -----------------------------\n",
    "def detect_patid3(transactions_df):\n",
    "    detections = []\n",
    "\n",
    "    filtered_df = transactions_df.dropna(subset=['merchant', 'gender'])\n",
    "    filtered_df['gender'] = filtered_df['gender'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    gender_counts = filtered_df.groupby(['merchant', 'gender'])['customer'].nunique().reset_index(name='count')\n",
    "\n",
    "    pivot_df = gender_counts.pivot(index='merchant', columns='gender', values='count').fillna(0).reset_index()\n",
    "    pivot_df.columns.name = None\n",
    "\n",
    "    if 'F' not in pivot_df.columns:\n",
    "        pivot_df['F'] = 0\n",
    "    if 'M' not in pivot_df.columns:\n",
    "        pivot_df['M'] = 0\n",
    "\n",
    "    eligible = pivot_df[(pivot_df['F'] > 100) & (pivot_df['F'] < pivot_df['M'])]\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    for _, row in eligible.iterrows():\n",
    "        merchant_id = row['merchant']\n",
    "        if merchant_id in already_detected_pat3:\n",
    "            continue\n",
    "\n",
    "        detections.append({\n",
    "            \"YStartTime\": now,\n",
    "            \"detectionTime\": now,\n",
    "            \"patternId\": \"PatId3\",\n",
    "            \"ActionType\": \"DEI-NEEDED\",\n",
    "            \"customerName\": \"\",\n",
    "            \"MerchantId\": merchant_id\n",
    "        })\n",
    "        already_detected_pat3.add(merchant_id)\n",
    "\n",
    "    return pd.DataFrame(detections)\n",
    "\n",
    "# -----------------------------\n",
    "# 💾 Write Detections\n",
    "# -----------------------------\n",
    "def write_detections(detections, output_dir=\"outputs/\", file_prefix=\"detections\"):\n",
    "    if detections.empty:\n",
    "        return\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i in range(0, len(detections), 50):\n",
    "        batch = detections.iloc[i:i+50]\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "        output_path = f\"{output_dir}/{file_prefix}_{timestamp}.csv\"\n",
    "        batch.to_csv(output_path, index=False)\n",
    "        print(f\"✅ Wrote {len(batch)} detections to {output_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 🚀 Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    chunk_folder = r\"C:\\Users\\kusha\\OneDrive\\Desktop\\Projects\\DevDolphins\\Blob files\\Chunks\"\n",
    "    all_files = sorted(os.listdir(chunk_folder))\n",
    "\n",
    "    for file in all_files:\n",
    "        print(f\"\\n📦 Processing: {file}\")\n",
    "        chunk_df = pd.read_csv(os.path.join(chunk_folder, file))\n",
    "        chunk_df['amount'] = chunk_df['amount'].astype(float)\n",
    "        chunk_df['gender'] = chunk_df['gender'].astype(str).str.strip().str.upper()\n",
    "\n",
    "        process_chunk(chunk_df)\n",
    "\n",
    "        detections1 = detect_patid1()\n",
    "        detections2 = detect_patid2()\n",
    "        detections3 = detect_patid3(chunk_df)\n",
    "\n",
    "        write_detections(detections1, file_prefix=\"detections_pat1\")\n",
    "        write_detections(detections2, file_prefix=\"detections_pat2\")\n",
    "        write_detections(detections3, file_prefix=\"detections_pat3\")\n",
    "\n",
    "# -----------------------------\n",
    "# 🔁 Entry Point\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c8b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
