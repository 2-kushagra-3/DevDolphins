{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb50d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.devdolphinstorage.dfs.core.windows.net\", \n",
    "    \"key\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import io\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_file = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5671ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup GDrive connection (already shared)\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "        service_account_file,\n",
    "        scopes=[\"https://www.googleapis.com/auth/drive\"]\n",
    "    )\n",
    "service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Step 2: Define your folder and filenames\n",
    "folder_id = \"1qryhdlgNsmecWRy2haI8S3uC63wKk5X-\"\n",
    "transactions_file = \"transactions.csv\"\n",
    "customer_file = \"CustomerImportance.csv\"\n",
    "\n",
    "# Step 3: Download both files\n",
    "def read_csv_from_gdrive(service, file_id):\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    file_buffer = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(file_buffer, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        _, done = downloader.next_chunk()\n",
    "    file_buffer.seek(0)\n",
    "    pandas_df = pd.read_csv(file_buffer)\n",
    "    return spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Get file IDs\n",
    "transactions_id = get_file_id(service, folder_id, transactions_file)\n",
    "customer_id = get_file_id(service, folder_id, customer_file)\n",
    "\n",
    "# Read as PySpark DataFrames\n",
    "transactions_df = read_csv_from_gdrive(service, transactions_id)\n",
    "customer_df = read_csv_from_gdrive(service, customer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# === Config ===\n",
    "base_path = \"abfss://gdrive-ingest@devdolphinstorage.dfs.core.windows.net\"\n",
    "transactions_path = f\"{base_path}/transactions\"\n",
    "reference_path = f\"{base_path}/reference_data/customer_importance\"\n",
    "\n",
    "# === Function to rename part file directly in transactions folder ===\n",
    "def rename_and_flatten_chunk(temp_dir: str, final_path: str):\n",
    "    files = dbutils.fs.ls(temp_dir)\n",
    "    for file in files:\n",
    "        if file.name.startswith(\"part-\") and file.name.endswith(\".csv\"):\n",
    "            dbutils.fs.mv(file.path, final_path)\n",
    "        else:\n",
    "            dbutils.fs.rm(file.path)\n",
    "    dbutils.fs.rm(temp_dir, recurse=True)\n",
    "\n",
    "# === Step 1: Write Reference Data ===\n",
    "customer_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(reference_path)\n",
    "print(f\"Uploaded customer importance data to: {reference_path}\")\n",
    "\n",
    "# === Step 2: Write transactions as flat CSVs directly in /transactions ===\n",
    "chunk_size = 10000\n",
    "total_rows = transactions_df.count()\n",
    "num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Add row number\n",
    "window_spec = Window.orderBy(monotonically_increasing_id())\n",
    "transactions_df = transactions_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_time = time.time()\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size\n",
    "    \n",
    "    chunk_df = transactions_df.filter((transactions_df.row_num > start) & (transactions_df.row_num <= end)).drop(\"row_num\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    temp_chunk_dir = f\"{transactions_path}/tmp_chunk_{i+1}_{timestamp}\"\n",
    "    final_file_path = f\"{transactions_path}/chunk_{i+1}_{timestamp}.csv\"\n",
    "    \n",
    "    # Write chunk to temp location\n",
    "    chunk_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_chunk_dir)\n",
    "    \n",
    "    # Move part file to final flat location\n",
    "    rename_and_flatten_chunk(temp_chunk_dir, final_file_path)\n",
    "    \n",
    "    print(f\"Uploaded chunk {i+1}/{num_chunks} as {final_file_path}\")\n",
    "    \n",
    "    # Wait 1 second\n",
    "    elapsed = time.time() - start_time\n",
    "    sleep_time = max(0, 1.0 - elapsed)\n",
    "    time.sleep(sleep_time)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
